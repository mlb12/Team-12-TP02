---
title: "Logistic Regression Example Corrected"
author: "Team 12"
date: "4/19/2022"
output: word_document
---
The author imported the application_train dataset and did brief exploration.

```{r}
###  Reading in the training dataset - I only use one file for my model
#setwd("C:/Tommy/Kaggle/HomeCredit")
application_train = read.csv('application_train.csv', stringsAsFactors = FALSE)

###I want TARGET to be a factor
application_train$TARGET <- as.factor(application_train$TARGET)

###Only use clean columns
data <- application_train[c(2:21)]
summary(data)
#Note - TARGET has an incidence rate of .0807 - just over 8% of the records are 1
```

The author split data into train and test sets, but never used these sets in his models. Not sure if he forgot or changed his mind.

It is very important to perform cross validation to identify overfitting in the trained model, which can be a common issue in logistic regression.

```{r}
## Logic here for splitting dataset to train and validate
## I was using a small percentage of the data - I do this in the early stages to deal with memory issue
#Not sure if this is random - but not an issue - I just use the whole dataset
indexes = sample(1:nrow(data), size=0.2*nrow(data))
test = data[indexes,]
dim(test)
train = application_train[-indexes,]
```

The author evaluated a model with all features and used the importance report to determine significance.

```{r}
# Look at a logistic regression using all the remaining columns to see what is significant
model <- glm(TARGET~., data=data, family="binomial")
summary(model)

# Estimate variable importance
library(caret)
importance <- data.frame(varImp(model, scale=TRUE))
importance
```

The author did not check assumptions for this model before creating his final model, so Team 12 added the following chunk of code. In terms of multicollinearity, days_employed had a very high GVIF score, followed by amt_goods_price and amt_credit, which were just above 5. Additionally, we tested the linearity of the logit assumption for one continuous variable, as an example, and it violated the assumption. While logistic regression is not as strict with assumptions as linear regression, it is still necessary to consider these steps when developing your final model.

```{r}
#Checking for Multicollinearity
library(car)
car::vif(model)

#Checking for Linearity of the Logit
#One example of a continuous independent variable
attach(data)
plot(AMT_CREDIT, log(AMT_CREDIT))
interaction <- AMT_CREDIT*log(AMT_CREDIT)
modelassumptions <- glm(TARGET~interaction, data=data, family="binomial")
#interaction term is statistically significant, which means assumption is violated
summary(modelassumptions)

```

Using his original model and importance report, the author narrowed down his model to just 4 features, which we think limits its predictive ability. Based on the above analysis, there appears to be numerous other potentially significant variables, including education_type, family_status, car ownership, and age, which were excluded from the model.

Another problem we encountered was that the confusion matrix output was confusing (no pun intended). Since the author did not use an if-else statement and simply put an inequality statement in the table, the output column names for actual (0, 1) did not align with the predicted column names (False, True). It was unclear if probabilities above the 0.08 threshold represented 1 or 0 and same for below the threshold, which made the confusion matrix difficult to understand.

```{r pressure, echo=FALSE}
#No formulaic approach - I just looked at the statistically significant variables with the highest importance
# Use those variables to calculate a confusion matrix
#Use 8.07% as cutoff - this is population average
model2 <- glm(TARGET~ CODE_GENDER + AMT_CREDIT + AMT_GOODS_PRICE + DAYS_EMPLOYED, data=data, family="binomial")
summary(model2)
prediction <- predict(model2, data, type = "response")
head(prediction)
summary(prediction)

confusion <- table(data$TARGET, prediction >= 0.0807)
confusion
## Team 12 added this line of code to examine actual predictions
(confusion[1] + confusion[4])/sum(confusion)
```

As mentioned previously, the author completely skipped the cross validation step and went directly to making predictions on the application_test set, which did not include labels. Here is where we confirmed the correct labels for probabilities above and below 0.08. Then, the author wrote out his model's predictions to a CSV file for the competition submission.

```{r}
##Now make predictions on the testing data set

application_test = read.csv('application_test.csv', stringsAsFactors = FALSE)
prediction <- predict(model2, application_test, type = "response")
summary(prediction)
head(prediction)

submission <- cbind(application_test, prediction)
submission<- submission[c(1,122)]

submission$TARGET <- ifelse(submission$prediction>0.0823,1,0)
submission<- submission[c(1,3)]
head(submission)

write.csv(submission,file = "submission.csv", row.names = FALSE)
```

